#+TITLE:       Hadoop for DevOps
#+AUTHOR:      Steven Byrnes
#+BEGIN_HTML
<p>Type <strong>T</strong> to begin the slide show.</p>
#+END_HTML

# Goal:
# 10 slides
# What are some of the issues:
# - capturing lots of data
# - in some cases, need to process it real time, or semi-real time;
#   explore the data real time
#
# Some possible products
# - OpenTSDB
# - Scribe
# - Flume
# Demos - 3
# Conclusion


## C-c C-e

# cp -vip ~/Software/org-html-slideshow/production/{org-html-slideshow.js,common.css,presenter.css,projection.css,screen.css} .

# Logging: Scribe vs Flume vs Kafka.  Or logstash, then feed into OpenTSDB that way?
# Statistics
# Alerting?

# http://www.infoq.com/presentations/Hadoop-HDFS-Facebook

# Include sketches in my presentations?

# Goal: 40-45 minute presentation

# This is about leveraging big data technologies, specifically those
# around the Hadoop ecosystem, for dealing with the large quantity of
# data generated by the servers themselves.  Varies between
# structured, semi-structured, and unstructured.  Traditional tools
# might work when you have dozens, or hundreds of servers.  But what
# if you have thousands.  Or you just are generating data points at a
# high rate.  Like sensor data.


# a lot of products... like so many tinkertoys...  not always plug and
# play...  at each layer, multiple open source products.

# metrics: opentsdb.  doesn't address the metric extraction, but it
# takes very simple format; so easy can do it in a 1 line script.
#
# logs: a bunch of stuff.  extracting metrics from logs: a lot of stuff.


* Hadoop for DevOps

** notes                :notes:

Title is ambiguous?




* Entre                                                               :slide:

** Notes    :notes:

We will give covering a few tools that directly or indirectly can be
used to process system level data - logs and system measurements - and
store them in Hadoop or HBase.



** We need more data(points)!                                        :slide:

** Graphing data like it's 1998!                                     :slide:

#+ATTR_HTML: style="float:right;"
[[./img/mrtg.png]]

MRTG. Awesomeness!

RRD: awesomeness... once you get into it.

*** notes                :notes:

You see it everywhere.  (Show pfsense box screen cap)

http://static.usenix.org/publications/library/proceedings/lisa98/full_papers/oetiker/oetiker.pdf

** Logging like it's 1988!                                           :slide:

I'm a lumberjack, and I use syslog.  Except...

#+begin_example
$ wc -lc *.20130315
9,342,775 1,199,685,316
#+end_example

(That's lines and characters.  For one server.  For one day.)

** Great, but now what? (graphs)                                     :slide:

*** Problems                                                        :slide:

- static in timeliness (have to regen)
- lossy database (space tradeoff. fixed intervals set when you create it.)


*** So you get this...                                              :slide:

[[./img/status_rrd_graph_img_1day.png]]

*** Or this...                                                      :slide:

[[./img/status_rrd_graph_img_1week.png]]


*** notes                                                :notes:

But supposed you want a month periods?  And you didn't specify it when
you created the RRD?

Bummer...

*** Has this happened to you?                                       :slide:

prod issue lands on your desk. they want you to look into a problem.
requires 1 minute granularity from last week? LOLZ! (have LOLZCAT
image)

(or laughing horse)
  
*** What we would like                                              :slide:

- more data
- more flexible graphing.
  I want to see my web request latency vs my database latency.  And
  maybe some other metric combo I just thought of
- basically, more cowbell!


* Logs?                                                               :slide:

Problems: data locked in there.  For example, web access logs.  Those
are handy, and have their own tools.  But suppose you have other
things... diagnostics.  Errors.   Might want to track those.

Textual stuff you want to search or act on.

Numeric information, or information you can turn into numbers, that
you want to graph.

* Splunk?

A good solution, from what I've heard, to certain issues.

* There are tools out there

But say you need to track thousands of metrics across hundreds or
thousands of machines.  Some every second.  And let's not even talk
about all of the logs we're generating!

BURN!

(kelso)
  
* Hadoop to the rescue                                                :slide:

Big data processing!

Meta-irony: if you have sufficiently large Hadoop, or other, cluster,
it might be driving your needs to track lots of things.

* OpenTSDB to the rescue                                              :slide:

- Runs on HBase
- No downsampling
- Originally written by [[http://stumbleupon.com/][StumbleUpon]]
- Written to store and query metrics across clusters
- Large scale
  - Hundreds of thousands of time series
  - Billions datapoints... per day!


** OpenTSDB users                                                    :slide:

Box
Tumblr
StumbleUpon

** OpenTSDB Components                                              :slide:

[[file:tsdb-architecture.png]]

(from opentsdb.net)

*** notes                :notes:

tsd = Time Series Daemon

writes the metrics to HBase.
provides web interface for querying


** Other component

tcollector

talks to =tsd=

will runs scripts, de-dup, send to tsd

** What is a metric?

- name
- Unix timestamp (second resolution; millisecond in forks and next
  version)
- value (64 bit integer or floating point number)
- 0 or more key-value tags

Example:

#+begin_quote
mysql.connections 123400000 200 host=db1 env=production
#+end_quote

Anything that can be expressed as a number can be a metric.
Databases, number of users, network statistics, etc.


** Demo

Start (single node) HBase:

#+begin_quote
export HADOOP_OPTS="-Djava.security.krb5.realm= -Djava.security.krb5.kdc="
./bin/start-hbase.sh 
#+end_quote

Create tables if necessary:

#+begin_quote
env COMPRESSION=none HBASE_HOME=../hbase-0.94.4 ./src/create_table.sh
#+end_quote

Examine tables:

#+begin_quote
../hbase-0.94.4/bin/hbase shell
describe 'tsdb'
describe 'tsdb-uid'
#+end_quote

Start tsdb:

#+begin_quote
tsdtmp=${TMPDIR-'/tmp'}/tsd
mkdir -p "$tsdtmp"
./build/tsdb tsd --port=4242 --staticroot=build/staticroot --cachedir="$tsdtmp"
#+end_quote

(or ../hbase-example/start-tsdb)

Look at web interface:

#+begin_quote
me=192.168.6.201
sudo ifconfig en0 inet $me/32 alias
open http://192.168.6.201:4242
#+end_quote

Ok... now add some metrics:

#+begin_quote
./build/tsdb mkmetric some.stat1 some.stat2
#+end_quote

(show auto completion in web interface)

or a bunch for self monitoring:

#+begin_quote
echo stats | nc -w 1 $me 4242 \
| awk '{ print $1 }' | sort -u \
| xargs ./build/tsdb mkmetric
#+end_quote

Now load up data:

#+begin_quote
../hbase-examples/genstats.pl | nc -w 2 $me 4242
#+end_quote

Look at interface for past 10 minutes.  Now, load up a lot!

#+begin_quote
../hbase-examples/genstats2.pl| wc -l
1,209,602

../hbase-examples/genstats2.pl| nc -w 1 $me 4242

../hbase-examples/genstats.pl|nc -w 1 $me 4242

while true                                          
do
 ../hbase-examples/genstats.pl|nc -w 1 $me 4242
 sleep 4
done
#+end_quote



*** notes                :notes:

(perhaps hitting an AWS cluster?)


** TODO demo

tcollector?

* Now what about those logs?                                          :slide:

(image of redwood logs?)

** What is in them?                                                  :slide:

- Analytic information
- Errors
- Other metrics

** For instance?                                                     :slide:

#+begin_example
12:33:45 ERROR Your developer was smoking crack!  Fail!
12:33:46 EVENT 200 widgets were frobnicated in 30 ms
#+end_example

** There's gold in them thar logs!

** How to process?

** Capturing the logs                                                :slide:

Pick your poison

** Scribe                                                            :slide:

- Started by Facebook
- Log aggregation
- Doesn't handle putting data into HBase
- Needs other tools
- https://github.com/facebook/scribe

** Flume                                                             :slide:

[[file:./img/flume-diagram.png]]

- Apache project.  Started by Cloudera
- Sources and sinks
- Sources include files, syslog, network port, Avro, and Scribe!
- Sinks include Avro, HDFS, HBase, ElasticSearch, and IRC!
- http://flume.apache.org/

** Kafka                                                             :slide:

- Apache project.  Started by LinkedIn
- Pub sub messaging
- Used to move activity stream data (i.e. what's in logs) into Hadoop
- Out of box, needs code or log4j config
- http://kafka.apache.org/

** Other solutions                                                   :slide:

(not comprehensive list)

- Logstash
- Logster
- StatsD

Don't necessarily directly interop with HDFS, HBase, or OpenTSDB, but have
plugins or otherwise can feed data.

*** Solutions                                :notes:

A whole lot of role your own.

Facebook does it.

Petabytes of data.

** Demo 1 - Flume                                                     :slide:

#+begin_src dot :file flume-to-hbase.png :cmdline -Kdot -Tpng
digraph G {
  rankdir=LR
  Logs -> Channel
  Channel -> HBase
}
#+end_src

Configuration:

#+begin_quote
cat ~/examples/demo-flume/flume-hbase.conf
#+end_quote

#+begin_quote
./bin/flume-ng agent -C $CP --conf-file ~/examples/demo-flume/flume-hbase.conf --name a1 --conf ./conf
#+end_quote

Let's look at HBase:

#+begin_quote
../hbase-0.94.5/bin/hbase shell
scan 'log_table'
#+end_quote

*** notes                                                             :notes:

Elasticsearch could be its own topic.   Provides a friendly search
layer over Lucene.  Supports its own clustering.

Simple single agent.

Source is a spool directory.  I put a single Apache log file in it.

The sink is an HBase table.  It doesn't do any parsing of fields.
There are serializers that do that for you.

** Demo 2                                                            :slide:

#+ATTR_HTML: style="float:right;"
[[./img/logstash.png]]

Logstash is swiss army knife of logging.  Maybe a bit too much.

** Demo 2 (setup)                                                    :slide:


#+begin_src dot :file logs.png :cmdline -Kdot -Tpng
digraph G {
  rankdir=LR
  Server1 -> Logster
  Server2 -> Logster
  Server3 -> Logster
  ServerN -> Logster

  Logster -> Elasticsearch
  Logster -> OpenTSDB
  Logster -> other...
}
#+end_src

*** notes                 :notes:

- Logs go to Logsters
- Send raw logs to Elasticsearch, which can be its own cluster.  Fancy searching.
- Logstash pipes metrics to OpenTSDB.

We get post hoc searching of logs.

We extract useful bits from it for analysis.

Alerting?  t.b.d.

label="Logs setup"


commands:

#+begin_quote
java -jar logstash-1.1.9-monolithic.jar agent -f logstash.conf -- web --backend 'elasticsearch:///?local'
#+end_quote

then, check the web site: [[http://localhost:9292/]]

Search for "user", "error", "invalid user".


* Logs: Statsd, Logster, Logstash                                     

* Some heading                                                        :slide:
* For more info                                :slide:

** Logs

*** Flume

http://flume.apache.org/releases/content/1.3.1/FlumeUserGuide.pdf
seems more up to date...

http://mapredit.blogspot.de/2012/06/apache-flume-12x-and-hbase.html
http://www.slideshare.net/mapredit/flume-and-hbase

*** Case Studies

http://www.infoq.com/presentations/Hadoop-HDFS-Facebook

* Resources                 :noexport:


http://opentsdb.net/faq.html

how much space?

100B points =~ 1TB.  12 bytes/data point.

create a graph... Hadoop.  HBase.  OpenTSDB.  TSD.  tcollector.  



[[http://hadoop.apache.org/][(Apache) Hadoop]]: implements map/reduce infrastructure on top of
commodity hardware. Inspired by Google [[http://static.usenix.org/event/osdi04/tech/full_papers/dean/dean.pdf][MapReduce: Simpliﬁed Data
Processing on Large Clusters]] paper, computational jobs are dividend
into pieces and distributed across a cluster. Results are then
collected, and aggregated. It can be run in single host mode. 

Hadoop Filesystem (HDFS).  A distributed filesystem built to work with
Hadoop's map/reduce framework.

Hadoop is used by, or solutions available from, Yahoo!, Facebook,
Amazon, Apple, StumbleUpon, IBM, Microsoft, etc.  Facebook runs a
single HDFS cluster with over [[http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920][100PB]] of storage.


HBase is a non-relational database built on top of HDFS. Example
deployment: Facebook's messaging system is built on top of HBase.
Provides a number of methods for accessing the data.  The native HBase
shell is Ruby based.  But with Hive (see below), you can add a
SQL-like interface.

[[http://hive.apache.org/][Apache Hive]]: a data warehousing solution that runs on top of Hadoop
and HBase.  It uses a SQL-like querying language.


[[http://cloudera.github.com/hue/][HUE]] - a tool for browsing Hadoop.

[[http://opentsdb.net/][OpenTSDB]] - open time series database.  Built on top of HBase.  Written
by StumbleUpon and used by a [[https://github.com/OpenTSDB/opentsdb/wiki/Companies-using-OpenTSDB-in-production][number of companies]].



OpenTSDB architecture

http://opentsdb.net/img/tsdb-architecture.png

metrics written to TSD: Time Series Daemon.  Metrics can be fed
directly, or run by tcollector.  TSD writes to HBase.


Each data point has:
- a metric name
- a Unix timestamp (second granularity)
- a value (64 bit integer or double precision float)
- tags (key value pairs) to annotate the point

Tags can have anything, like host, environment (prod vs qa), etc. 

Advantages:
- can collect a large amount of data without downsampling


tcollector:
- runs collectors (which just need to output to stdout)
- de-duplicates data



But what if I don't want to save things forever?

tsdb scan --delete




Demo:


start (single node) HBase

export HADOOP_OPTS="-Djava.security.krb5.realm= -Djava.security.krb5.kdc="
./bin/start-hbase.sh 

create tables if necessary

env COMPRESSION=none HBASE_HOME=../hbase-0.94.4 ./src/create_table.sh
../hbase-0.94.4/bin/hbase shell
describe 'tsdb'
describe 'tsdb-uid'



start TSD


tsdtmp=${TMPDIR-'/tmp'}/tsd
mkdir -p "$tsdtmp"
./build/tsdb tsd --port=4242 --staticroot=build/staticroot --cachedir="$tsdtmp"

look at web interface

open http://192.168.4.162:4242/

DONE!



monitor self:

create statistics:

echo stats | nc -w 1 $me 4242 \
| awk '{ print $1 }' | sort -u \
| xargs ./build/tsdb mkmetric



ok, let's create a couple of new metrics.
and just so we have some day, let's populate some with a Perl script.

./build/tsdb mkmetric some.stat1 some.stat2

../genstats.pl | wc -l
172k points
../genstats.pl | nc -w 2 $me 4242


let's clean up

./build/tsdb scan --delete 2013/02/13-23:00 sum some.stat1










https://cwiki.apache.org/FLUME/home.html

"Apache Flume is a distributed, reliable, and available service for
efficiently collecting, aggregating, and moving large amounts of log
data. Its main goal is to deliver data from applications to Apache
Hadoop's HDFS. It has a simple and flexible architecture based on
streaming data flows. It is robust and fault tolerant with tunable
reliability mechanisms and many failover and recovery mechanisms. It
uses a simple extensible data model that allows for online analytic
applications."

http://www.quora.com/What-are-the-key-differences-between-Flume-and-Scribe/answer/Jack-Stahl

centralized configuration.  centralized liveness monitoring.  

https://github.com/facebook/scribe

"Scribe is a server for aggregating log data that's streamed in real
time from clients. It is designed to be scalable and reliable."

http://stackoverflow.com/questions/12559570/flume-vs-kafka-vs-others

kafka?  http://www.mediawiki.org/wiki/Analytics/Kraken/Request_Logging
http://kafka.apache.org/



https://github.com/etsy/logster

https://github.com/etsy/statsd/

http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/
measure at 3 levels: network, machine, and application

http://logstash.net/docs/1.1.5/tutorials/metrics-from-logs


http://ibmdatamag.com/2012/05/why-log-analytics-is-a-great-and-awful-place-to-start-with-big-data/
"Log processing (for ad placement) is a core use case that Hadoop was
invented to help with—so it’s no surprise that it functions well in
this scenario."

http://help.papertrailapp.com/kb/analytics/log-analytics-with-hadoop-and-hive
CREATE EXTERNAL TABLE events (
  id bigint, received_at string, generated_at string, source_id int, source_name string, source_ip string, facility string, severity string, program string, message string
)
PARTITIONED BY (
  dt string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION 's3://your-s3-bucket.yourdomain.com/papertrail/logs';



https://blogs.oracle.com/datawarehousing/entry/flume_and_hive_for_log
http://www.cubrid.org/blog/dev-platform/log-analysis-system-using-hadoop-and-mongodb/

http://www.tom-e-white.com/2008/01/hadoop-and-log-file-analysis.html

http://help.papertrailapp.com/kb/analytics/log-analytics-with-hadoop-and-hive

http://www.elasticsearch.org/

* to do                :noexport:


** Install all 3 on my AWS server (high)

scripts to do all of it

*** Install OpenTSDB
*** Install LogStash

stand alone.
have it read logs.
have it extract metrics and send to OpenTSDB

*** Install Flume

have it read logs
have it write to HBase
** bring my external monitor (medium-high)

** Better HTML slides (medium-high)
** Get Google Hangout working on laptop (medium)

** Get bluetooth headset working with laptop (low)

** Logs - Flume

** DONE Logs - Logstash
CLOSED: [2013-04-02 Tue 01:05]
- State "DONE"       from ""           [2013-04-02 Tue 01:05]

get started with logstash:

steve.net
vps
liquidfare.com?
running on local machine

** OpenTSDB



* Now what?                                                           :slide:

Future stuff:
- hook into alerting.  Pagerduty?  Zenoss?  Nagios?  Ganglia.  Riemann
- CEP
- machine learning, statistics


* Resources                                                           :slide:

Flume
Scribe
HBase
Logstash

[[http://cuddletech.com/blog/?p=795][Hadoop Analysis of Apache Logs Using Flume-NG, Hive and Pig]]

* The End                                                             :slide:

** notes :notes:

We did a high level overview of some of the tools available for
processing logs and analyzing statistics.

I've focused on some of the more low level tools that directly talk
with Hadoop / HBase.  There are tools that have been adapted to work
on top of these and provide a nicer front end.  Things like Graphite,
which does graphing.

Since the data is in Hadoop, you can then process it with tools like
Pig, Hive, Cascalog, or even R.  

Mostly, these aren't plug in play solutions. They're more like tinker
toys: you get a bunch of pieces. What you build is up to you.

We mostly looked at the analisys end.  Traditional system monitor
tools also perform alerting.  Plugging in tools into the messaging
buses some of these tools support, like Avro for Flume or Redis for
Logstash, one could construct very complicated real-time workflows to
deal with large clusters.

#+TAGS: slide(s)

#+STYLE: <link rel="stylesheet" type="text/css" href="common.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="screen.css" media="screen" />
#+STYLE: <link rel="stylesheet" type="text/css" href="projection.css" media="projection" />
#+STYLE: <link rel="stylesheet" type="text/css" href="presenter.css" media="presenter" />

#+BEGIN_HTML
<script type="text/javascript" src="org-html-slideshow.js"></script>
#+END_HTML

# Local Variables:
# org-export-html-style-include-default: nil
# org-export-html-style-include-scripts: nil
# buffer-file-coding-system: utf-8-unix
# End:
